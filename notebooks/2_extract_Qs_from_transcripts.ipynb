{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24b5728-8ad2-46df-b129-a9101d60b1c9",
   "metadata": {},
   "source": [
    "## Review each transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9518a280-44d1-4cd9-8073-620abacc1b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68cbc03457d4ee8b5a99fbd39218c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc2f7dc1ccb44dab79f0c5393ab041a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57db1edfe47b4557b4125f925a9d0dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c761f27495904f0fac0f3bfbda44a2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba98896a2694687b3105792cf1f516e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "base_path = '../output'\n",
    "candidate_folders = ['bullrich', 'massa', 'milei', 'schiaretti', 'bregman']\n",
    "\n",
    "# store all transcribed phrases for each candidate\n",
    "transcriptions = {candidate: [] for candidate in candidate_folders}\n",
    "\n",
    "# let's write a function that takes the 'segments' array and the candidate's name as inputs and returns the desired long text.\n",
    "\n",
    "def generate_long_text(segments, candidate_name):\n",
    "    long_text = []\n",
    "    for segment in segments:\n",
    "        speaker = \"Host\" if not segment['is_candidate'] else candidate_name\n",
    "        text = f\"{speaker}: {segment['text'].strip()}\"\n",
    "        long_text.append(text)\n",
    "    return long_text\n",
    "\n",
    "for candidate in candidate_folders:\n",
    "    folder_path = os.path.join(base_path, candidate)\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "            with open(filepath, 'r') as f:\n",
    "                content = json.load(f)\n",
    "                \n",
    "                # Generate full text if segments are available\n",
    "                segments = content.get('segments', []) if isinstance(content, dict) else []\n",
    "                full_text = generate_long_text(segments, candidate)\n",
    "                content['full_text'] = ' '.join(full_text)\n",
    "\n",
    "                # Add full filepath\n",
    "                content['filepath'] = filepath\n",
    "\n",
    "                if isinstance(content, dict):\n",
    "                    transcriptions[candidate].append(content)\n",
    "                elif isinstance(content, list):\n",
    "                    # Note: This assumes that the list contains dict items\n",
    "                    for entry in content:\n",
    "                        segments = entry.get('segments', [])\n",
    "                        full_text = generate_long_text(segments, candidate)\n",
    "                        entry['full_text'] = ' '.join(full_text)\n",
    "                        entry['filepath'] = filepath\n",
    "                    transcriptions[candidate].extend(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9290c3f7-5a8f-43d6-9409-a5d87d5ab725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate: bullrich, len: 219\n",
      "candidate: massa, len: 263\n",
      "candidate: milei, len: 1025\n",
      "candidate: schiaretti, len: 90\n",
      "candidate: bregman, len: 111\n"
     ]
    }
   ],
   "source": [
    "for each in candidate_folders:\n",
    "    print(f\"candidate: {each}, len: {len(transcriptions[each])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee0822dc-9e36-422f-adbf-35837c1e311d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: Buenas tardes a todos, buenas tardes a Vedra. Host: ¿Cómo están? Host: Gracias a todos por venir. Host: Esta es una de las excepciones de esta campaña, que es tener la oportunidad de escuchar a un candidato dando clases, enseñándonos sobre lo que es su materia de conocimiento. Host: Les quiero agradecer a todos que ustedes vengan acá a escucharnos, Javier, que estén presentes, que estén apoyando a la Libertad Avanza. Host: Este es un camino que comenzó mucho antes del 12 de septiembre con las militancias que llevamos cada uno en nuestros temas y que se va a coronar el 14 de noviembre con la ayuda de todos ustedes. Host: Por esa razón quiero pedirles que nos ayuden a fiscalizar, a cuidar los votos, Host: a que no nos roben, a que se escuche la voluntad ciudadana, a que se escuche nuestra voz y nuestras ideas. Host: No quiero que seamos la Argentina silenciada, no quiero que seamos la Argentina invisible, están nosotros que nos escuchen, que sepan lo que pensamos los argentinos, qu\n"
     ]
    }
   ],
   "source": [
    "candidate_name_test = \"milei\"\n",
    "print(transcriptions[candidate_name_test][100]['full_text'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c865d97-a077-459e-8b76-3cb7de9c62e5",
   "metadata": {},
   "source": [
    "## Now build a FAISS index, do a similarity search, and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "078db5c3-9e37-482f-adc7-1fda1f6a1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge faiss-gpu --y\n",
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c7d4f-172d-4cae-941a-d3f5f081496a",
   "metadata": {},
   "source": [
    "First, let's write a function to vectorize the text segments. We'll use sentence transformers for this example, but you can plug in whatever method you're comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b832e3c-ccdf-4470-bfc4-bbdb48468b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def vectorize_segments(segments):\n",
    "    texts = [segment['text'].strip() for segment in segments]\n",
    "    return model.encode(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055a201-cff4-439b-b5e8-566ee829e2a3",
   "metadata": {},
   "source": [
    "Now, let's install and import FAISS, and create a function to build an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2170dcd6-2aed-42ce-89e4-7e8e01c1c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# def build_faiss_index(vectors):\n",
    "#     dimension = vectors.shape[1]\n",
    "#     index = faiss.IndexFlatL2(dimension)\n",
    "#     index.add(vectors)\n",
    "#     return index\n",
    "\n",
    "def build_faiss_index(vectors):\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Move to GPU\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "    \n",
    "    gpu_index.add(vectors)\n",
    "    return gpu_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbef88e-d485-478d-96cc-2d96f6ce5498",
   "metadata": {},
   "source": [
    "Next, a function to find and remove duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a1773dd-e483-4337-8ed0-266ab43ca49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def remove_duplicates(candidate_segments):\n",
    "#     # Step 1: Vectorize all segments\n",
    "#     vectors = vectorize_segments(candidate_segments)\n",
    "\n",
    "#     # Step 2: Build FAISS index\n",
    "#     index = build_faiss_index(np.array(vectors))\n",
    "\n",
    "#     # Step 3: Query to find duplicates\n",
    "#     unique_segments = []\n",
    "#     threshold_distance = 0.1  # You can tune this\n",
    "\n",
    "#     for i, vec in enumerate(vectors):\n",
    "#         vec = np.expand_dims(vec, axis=0)\n",
    "#         distances, indices = index.search(vec, 2)  # 2 because the query vector itself will always be returned\n",
    "#         if distances[0][1] > threshold_distance:\n",
    "#             unique_segments.append(candidate_segments[i])\n",
    "\n",
    "#     return unique_segments\n",
    "\n",
    "def vectorize_full_texts(full_texts):\n",
    "    return model.encode(full_texts)\n",
    "\n",
    "def remove_duplicates(candidate_entries):\n",
    "    # Extract full_texts and vectorize them\n",
    "    full_texts = [entry.get('full_text', '') for entry in candidate_entries]\n",
    "    print(f\"Debug: Number of full_texts: {len(full_texts)}\")  # Debug\n",
    "\n",
    "    vectors = vectorize_full_texts(full_texts)\n",
    "    print(f\"Debug: Number of vectors: {len(vectors)}\")  # Debug\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    # Build FAISS index\n",
    "    index = build_faiss_index(vectors)\n",
    "\n",
    "    # Query to find duplicates\n",
    "    threshold_distance = 0.5  # Tune this\n",
    "    D, I = index.search(vectors, 2)\n",
    "\n",
    "    unique_entries = []\n",
    "    added_indices = set()\n",
    "\n",
    "    for i, (distances, indices) in enumerate(zip(D, I)):\n",
    "        print(f\"Debug: distances={distances}, indices={indices}\")  # Debug\n",
    "\n",
    "        if i not in added_indices:\n",
    "            if distances[1] > threshold_distance:\n",
    "                unique_entries.append(candidate_entries[i])\n",
    "                added_indices.add(i)\n",
    "            else:\n",
    "                if indices[1] not in added_indices:\n",
    "                    added_indices.add(indices[1])\n",
    "\n",
    "    print(f\"Debug: Number of unique_entries: {len(unique_entries)}\")  # Debug\n",
    "    return unique_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036075f9-645a-4649-8d01-1e244d0eed45",
   "metadata": {},
   "source": [
    "Finally, let's modify your main loop to remove duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fe86c36a-01a9-4019-b425-be328a458639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate: bullrich, len: 0\n",
      "candidate: massa, len: 0\n",
      "candidate: milei, len: 0\n",
      "candidate: schiaretti, len: 0\n",
      "candidate: bregman, len: 0\n"
     ]
    }
   ],
   "source": [
    "for each in candidate_folders:\n",
    "    print(f\"candidate: {each}, len: {len(transcriptions_unique[each])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "78b231ad-bd05-4b7c-907d-6b0e2f43ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bullrich\n",
      "Processing massa\n",
      "Processing milei\n",
      "Processing schiaretti\n",
      "Processing bregman\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to vectorize full texts\n",
    "def vectorize_full_texts(full_texts):\n",
    "    return model.encode(full_texts)\n",
    "\n",
    "# Function to build a FAISS index\n",
    "def build_faiss_index(vectors):\n",
    "    if len(vectors) == 0:\n",
    "        print(\"Warning: Empty vectors array.\")\n",
    "        return None\n",
    "    try:\n",
    "        dimension = vectors.shape[1]\n",
    "    except IndexError as e:\n",
    "        print(f\"Error: Unexpected vectors shape {vectors.shape}. Expected a 2D array.\")\n",
    "        return None\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "# Function to remove duplicates\n",
    "def remove_duplicates(transcriptions):\n",
    "    unique_transcriptions = {}\n",
    "    \n",
    "    for candidate, entries in transcriptions.items():\n",
    "        print(f\"Processing {candidate}\")\n",
    "        \n",
    "        # Debug: Check if entries are populated\n",
    "        if len(entries) == 0:\n",
    "            print(f\"Warning: No entries for {candidate}.\")\n",
    "            continue\n",
    "\n",
    "        # Vectorize full texts\n",
    "        full_texts = [entry['full_text'] for entry in entries]\n",
    "\n",
    "        # Debug: Print first few full_texts\n",
    "        # print(f\"Debug: First few full_texts for {candidate}: {full_texts[:3]}\")\n",
    "        \n",
    "        vectors = vectorize_full_texts(full_texts)\n",
    "        \n",
    "        # Debug: Print first few vectors\n",
    "        # print(f\"Debug: First few vectors for {candidate}: {vectors[:3]}\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        index = build_faiss_index(np.array(vectors))\n",
    "\n",
    "        # Skip if index is None (usually means empty vectors array)\n",
    "        if index is None:\n",
    "            print(f\"Skipping {candidate} due to empty index.\")\n",
    "            continue\n",
    "        \n",
    "        # Search for duplicates\n",
    "        D, I = index.search(vectors, 2)\n",
    "        \n",
    "        unique_entries = []\n",
    "        added_indices = set()\n",
    "        \n",
    "        for i, (distances, indices) in enumerate(zip(D, I)):\n",
    "            if i not in added_indices:\n",
    "                unique_entries.append(entries[i])\n",
    "                added_indices.add(i)\n",
    "                added_indices.add(indices[1])\n",
    "                \n",
    "        unique_transcriptions[candidate] = unique_entries\n",
    "    \n",
    "    return unique_transcriptions\n",
    "\n",
    "# Now remove duplicates\n",
    "transcriptions_unique = remove_duplicates(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3757540f-990d-4025-95ba-abd2568268f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate: bullrich, len: 159\n",
      "candidate: massa, len: 177\n",
      "candidate: milei, len: 752\n",
      "candidate: schiaretti, len: 61\n",
      "candidate: bregman, len: 79\n"
     ]
    }
   ],
   "source": [
    "for each in candidate_folders:\n",
    "    print(f\"candidate: {each}, len: {len(transcriptions_unique[each])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbf38c-c586-43b2-b0b8-af696a9d8b60",
   "metadata": {},
   "source": [
    "### Original\n",
    "- candidate: bullrich, len: 219\n",
    "- candidate: massa, len: 254\n",
    "- candidate: milei, len: 1022\n",
    "- candidate: schiaretti, len: 87\n",
    "- candidate: bregman, len: 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2ec38576-3e03-4e5d-992c-57220c3abe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_unique_files(transcriptions_unique, output_folder=\"../output/unique\"):\n",
    "    \"\"\"\n",
    "    Copy unique transcription files to a new folder.\n",
    "\n",
    "    Parameters:\n",
    "    - transcriptions_unique: Dictionary containing unique transcriptions.\n",
    "    - output_folder: The root folder where unique files will be stored.\n",
    "    \"\"\"\n",
    "    for candidate, entries in transcriptions_unique.items():\n",
    "        candidate_unique_folder = Path(output_folder) / candidate\n",
    "\n",
    "        # Create candidate's unique folder if it doesn't exist\n",
    "        candidate_unique_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for entry in entries:\n",
    "            src_path = entry['filepath']\n",
    "            file_name = Path(src_path).name\n",
    "            dest_path = candidate_unique_folder / file_name\n",
    "\n",
    "            # Copy the file\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Usage\n",
    "# Assuming transcriptions_unique is the dictionary containing unique transcriptions for each candidate\n",
    "copy_unique_files(transcriptions_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c37da-df25-412e-9f6b-b98bddad3f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machinelearnear-dev]",
   "language": "python",
   "name": "conda-env-machinelearnear-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
